{
  "name": "PromptFlow - Process Single Item",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "process-single_4FEjn96sNr06KFE8KoiY3IwQMJxsMEFksAb7ntsurSA",
        "responseMode": "responseNode",
        "options": {}
      },
      "id": "e8ca23e7-0fbd-4991-ae44-4f8616668f08",
      "name": "Webhook",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 1,
      "position": [-144, -160],
      "webhookId": "promptflow-single"
    },
    {
      "parameters": {
        "jsCode": "// Get input data directly from webhook\nconst data = $input.first().json;\nconst body = data.body || data;\n\nconst jobId = body.jobId || 'unknown';\nconst systemPrompt = body.systemPrompt || '';\nconst dataItem = body.dataItem || '';\nconst userPrompts = body.userPrompts || [];\nconst settings = body.settings || {};\n\n// Format the first prompt (replace {input} placeholder)\nlet formattedPrompt = '';\nif (Array.isArray(userPrompts) && userPrompts.length > 0) {\n  formattedPrompt = String(userPrompts[0]).replace('{input}', dataItem);\n} else {\n  formattedPrompt = dataItem;\n}\n\n// Combine system prompt and user prompt\nconst fullPrompt = systemPrompt ? `${systemPrompt}\\n\\n${formattedPrompt}` : formattedPrompt;\n\nreturn [{\n  json: {\n    jobId: jobId,\n    systemPrompt: systemPrompt,\n    dataItem: dataItem,\n    userPrompts: userPrompts,\n    settings: settings,\n    formattedPrompt: fullPrompt\n  }\n}];"
      },
      "id": "9f431563-8d59-4f8f-a7d9-bdc27a5be7a3",
      "name": "Prepare Prompts",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [80, -160]
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "={{ $json.formattedPrompt }}",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "typeVersion": 1.7,
      "position": [528, -160],
      "id": "9d395bc4-5836-4b89-a069-546915c23c0c",
      "name": "Basic LLM Chain"
    },
    {
      "parameters": {
        "model": "grok-4-1-fast-non-reasoning",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatXAiGrok",
      "typeVersion": 1,
      "position": [600, 64],
      "id": "d3f23615-3778-4008-8315-a8a59a1ea4f8",
      "name": "xAI Grok Chat Model",
      "credentials": {
        "xAiApi": {
          "id": "bmzO5SuregEOFeq5",
          "name": "xAi account"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Get the LLM response\nconst llmResponse = $input.first().json.text || $input.first().json.response || '';\n\n// Get original data from previous nodes (passed through)\nconst jobId = $('Prepare Prompts').first().json.jobId;\nconst dataItem = $('Prepare Prompts').first().json.dataItem;\nconst formattedPrompt = $('Prepare Prompts').first().json.formattedPrompt || '';\n\n// Estimate token usage (roughly 4 characters per token)\nconst estimateTokens = (text) => Math.ceil((text || '').length / 4);\nconst promptTokens = estimateTokens(formattedPrompt);\nconst completionTokens = estimateTokens(llmResponse);\n\nreturn [{\n  json: {\n    jobId: jobId,\n    input: dataItem,\n    intermediates: [],\n    finalOutput: llmResponse,\n    tokenUsage: {\n      prompt: promptTokens,\n      completion: completionTokens\n    },\n    status: 'success'\n  }\n}];"
      },
      "id": "a818d185-1d04-4cf4-8538-5e660016be63",
      "name": "Aggregate Results",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [880, -160]
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ $json }}",
        "options": {}
      },
      "id": "25afac90-861a-4661-86b2-d3cecb252e5d",
      "name": "Respond to Webhook",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.1,
      "position": [1104, -160]
    }
  ],
  "connections": {
    "Webhook": {
      "main": [[{"node": "Prepare Prompts", "type": "main", "index": 0}]]
    },
    "Prepare Prompts": {
      "main": [[{"node": "Basic LLM Chain", "type": "main", "index": 0}]]
    },
    "Basic LLM Chain": {
      "main": [[{"node": "Aggregate Results", "type": "main", "index": 0}]]
    },
    "Aggregate Results": {
      "main": [[{"node": "Respond to Webhook", "type": "main", "index": 0}]]
    },
    "xAI Grok Chat Model": {
      "ai_languageModel": [[{"node": "Basic LLM Chain", "type": "ai_languageModel", "index": 0}]]
    }
  },
  "pinData": {},
  "active": true,
  "settings": {},
  "id": "1",
  "meta": {
    "templateCredsSetupCompleted": true
  }
}
