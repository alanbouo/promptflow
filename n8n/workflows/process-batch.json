{
  "name": "PromptFlow - Process Batch",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "process-batch",
        "responseMode": "lastNode",
        "options": {}
      },
      "id": "webhook-batch",
      "name": "Webhook",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 1,
      "position": [-144, -160],
      "webhookId": "promptflow-batch"
    },
    {
      "parameters": {
        "jsCode": "// Get input data directly from webhook\nconst data = $input.first().json;\nconst body = data.body || data;\n\nconst jobId = body.jobId || 'unknown';\nconst systemPrompt = body.systemPrompt || '';\nconst userPrompts = body.userPrompts || [];\nconst settings = body.settings || {};\nconst dataItems = body.dataItems || [];\nconst batchSize = body.batchSize || 5;\nconst callbackUrl = body.callbackUrl || '';\n\n// Create an item for each data item\nconst items = dataItems.map((item, index) => ({\n  json: {\n    jobId: jobId,\n    systemPrompt: systemPrompt,\n    userPrompts: userPrompts,\n    settings: settings,\n    dataItem: item,\n    itemIndex: index,\n    callbackUrl: callbackUrl,\n    batchSize: batchSize\n  }\n}));\n\nreturn items;"
      },
      "id": "split-items",
      "name": "Split Items",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [80, -160]
    },
    {
      "parameters": {
        "batchSize": "={{ $json.batchSize || 5 }}",
        "options": {}
      },
      "id": "batch",
      "name": "SplitInBatches",
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 3,
      "position": [304, -160]
    },
    {
      "parameters": {
        "jsCode": "// Get input data\nconst jobId = $input.first().json.jobId;\nconst systemPrompt = $input.first().json.systemPrompt;\nconst dataItem = $input.first().json.dataItem;\nconst userPrompts = $input.first().json.userPrompts || [];\nconst settings = $input.first().json.settings || {};\nconst itemIndex = $input.first().json.itemIndex;\nconst callbackUrl = $input.first().json.callbackUrl;\n\n// Format the first prompt (replace {input} placeholder)\nlet formattedPrompt = '';\nif (Array.isArray(userPrompts) && userPrompts.length > 0) {\n  formattedPrompt = String(userPrompts[0]).replace('{input}', dataItem);\n} else {\n  formattedPrompt = dataItem;\n}\n\n// Combine system prompt and user prompt\nconst fullPrompt = systemPrompt ? `${systemPrompt}\\n\\n${formattedPrompt}` : formattedPrompt;\n\nreturn [{\n  json: {\n    jobId: jobId,\n    systemPrompt: systemPrompt,\n    dataItem: dataItem,\n    userPrompts: userPrompts,\n    settings: settings,\n    itemIndex: itemIndex,\n    callbackUrl: callbackUrl,\n    formattedPrompt: fullPrompt\n  }\n}];"
      },
      "id": "prepare-prompts-batch",
      "name": "Prepare Prompts",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [528, -160]
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "={{ $json.formattedPrompt }}",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "typeVersion": 1.7,
      "position": [752, -160],
      "id": "llm-chain-batch",
      "name": "Basic LLM Chain"
    },
    {
      "parameters": {
        "model": "grok-4-1-fast-non-reasoning",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatXAiGrok",
      "typeVersion": 1,
      "position": [824, 64],
      "id": "grok-batch",
      "name": "xAI Grok Chat Model",
      "credentials": {
        "xAiApi": {
          "id": "bmzO5SuregEOFeq5",
          "name": "xAi account"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Get the LLM response\nconst llmResponse = $input.first().json.text || $input.first().json.response || '';\n\n// Get original data from previous nodes\nconst jobId = $('Prepare Prompts').first().json.jobId;\nconst dataItem = $('Prepare Prompts').first().json.dataItem;\nconst itemIndex = $('Prepare Prompts').first().json.itemIndex;\nconst callbackUrl = $('Prepare Prompts').first().json.callbackUrl;\n\nreturn [{\n  json: {\n    jobId: jobId,\n    itemIndex: itemIndex,\n    input: dataItem,\n    intermediates: [],\n    finalOutput: llmResponse,\n    callbackUrl: callbackUrl,\n    tokenUsage: {\n      prompt: 0,\n      completion: 0,\n      total: 0\n    },\n    status: 'success'\n  }\n}];"
      },
      "id": "aggregate-batch",
      "name": "Aggregate Results",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [976, -160]
    },
    {
      "parameters": {
        "jsCode": "// Collect all results from the batch\nconst allItems = $input.all();\n\nconst results = allItems.map(item => ({\n  itemIndex: item.json.itemIndex,\n  input: item.json.input,\n  intermediates: item.json.intermediates,\n  finalOutput: item.json.finalOutput,\n  tokenUsage: item.json.tokenUsage,\n  status: item.json.status\n}));\n\n// Sort by itemIndex\nresults.sort((a, b) => a.itemIndex - b.itemIndex);\n\nreturn [{\n  json: {\n    jobId: allItems[0].json.jobId,\n    results: results,\n    callbackUrl: allItems[0].json.callbackUrl,\n    status: 'success'\n  }\n}];"
      },
      "id": "merge-results",
      "name": "Merge Results",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1200, -160]
    },
    {
      "parameters": {
        "url": "={{ $json.callbackUrl }}",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ JSON.stringify($json) }}",
        "options": {}
      },
      "id": "callback",
      "name": "Callback to Next.js",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [1424, -160]
    }
  ],
  "connections": {
    "Webhook": {
      "main": [[{"node": "Split Items", "type": "main", "index": 0}]]
    },
    "Split Items": {
      "main": [[{"node": "SplitInBatches", "type": "main", "index": 0}]]
    },
    "SplitInBatches": {
      "main": [
        [{"node": "Prepare Prompts", "type": "main", "index": 0}],
        [{"node": "Merge Results", "type": "main", "index": 0}]
      ]
    },
    "Prepare Prompts": {
      "main": [[{"node": "Basic LLM Chain", "type": "main", "index": 0}]]
    },
    "Basic LLM Chain": {
      "main": [[{"node": "Aggregate Results", "type": "main", "index": 0}]]
    },
    "Aggregate Results": {
      "main": [[{"node": "SplitInBatches", "type": "main", "index": 0}]]
    },
    "Merge Results": {
      "main": [[{"node": "Callback to Next.js", "type": "main", "index": 0}]]
    },
    "xAI Grok Chat Model": {
      "ai_languageModel": [[{"node": "Basic LLM Chain", "type": "ai_languageModel", "index": 0}]]
    }
  },
  "pinData": {},
  "active": true,
  "settings": {},
  "id": "2",
  "meta": {
    "templateCredsSetupCompleted": true
  }
}
